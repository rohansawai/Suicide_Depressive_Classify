{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRAPING https://www.reddit.com/r/SuicideWatch.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1230\n",
      "Number of unique posts: 980\n",
      "LIST NOW CONTAINS 980 UNIQUE SCRAPED POSTS\n",
      "SCRAPING https://www.reddit.com/r/depression.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1243\n",
      "Number of unique posts: 992\n",
      "LIST NOW CONTAINS 992 UNIQUE SCRAPED POSTS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data analysis imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "# NLP Imports\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from random import randint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import wordninja\n",
    "\n",
    "# creating user agent\n",
    "headers = {\"User-agent\" : \"PsychologicalCat7089\"} # set user agent to reddit account username\n",
    "url_1 = \"https://www.reddit.com/r/depression.json\"\n",
    "\n",
    "res = requests.get(url_1, headers=headers)\n",
    "res.status_code\n",
    "\n",
    "# scraper function\n",
    "def reddit_scrape(url_string, number_of_scrapes, output_list):\n",
    "    #scraped posts outputted as lists\n",
    "    after = None \n",
    "    for _ in range(number_of_scrapes):\n",
    "        if _ == 0:\n",
    "            print(\"SCRAPING {}\\n--------------------------------------------------\".format(url_string))\n",
    "            print(\"<<<SCRAPING COMMENCED>>>\") \n",
    "            print(\"Downloading Batch {} of {}...\".format(1, number_of_scrapes))\n",
    "        elif (_+1) % 5 ==0:\n",
    "            print(\"Downloading Batch {} of {}...\".format((_ + 1), number_of_scrapes))\n",
    "        \n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            #THIS WILL TELL THE SCRAPER TO GET THE NEXT SET AFTER REDDIT'S after CODE\n",
    "            params = {\"after\": after}             \n",
    "        res = requests.get(url_string, params=params, headers=headers)\n",
    "        if res.status_code == 200:\n",
    "            the_json = res.json()\n",
    "            output_list.extend(the_json[\"data\"][\"children\"])\n",
    "            after = the_json[\"data\"][\"after\"]\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        time.sleep(randint(1,6))\n",
    "    \n",
    "    print(\"<<<SCRAPING COMPLETED>>>\")\n",
    "    print(\"Number of posts downloaded: {}\".format(len(output_list)))\n",
    "    print(\"Number of unique posts: {}\".format(len(set([p[\"data\"][\"name\"] for p in output_list]))))\n",
    "\n",
    "# remove any repeat posts\n",
    "def create_unique_list(original_scrape_list, new_list_name):\n",
    "    data_name_list=[]\n",
    "    for i in range(len(original_scrape_list)):\n",
    "        if original_scrape_list[i][\"data\"][\"name\"] not in data_name_list:\n",
    "            new_list_name.append(original_scrape_list[i][\"data\"])\n",
    "            data_name_list.append(original_scrape_list[i][\"data\"][\"name\"])\n",
    "    #CHECKING IF THE NEW LIST IS OF SAME LENGTH AS UNIQUE POSTS\n",
    "    print(\"LIST NOW CONTAINS {} UNIQUE SCRAPED POSTS\".format(len(new_list_name)))\n",
    "\n",
    "# scraping suicide_watch data\n",
    "suicide_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/SuicideWatch.json\", 50, suicide_data)\n",
    "\n",
    "suicide_data_unique = []\n",
    "create_unique_list(suicide_data, suicide_data_unique)\n",
    "\n",
    "# add suicide_watch to dataframe\n",
    "suicide_watch = pd.DataFrame(suicide_data_unique)\n",
    "suicide_watch[\"is_suicide\"] = 1\n",
    "suicide_watch.head()\n",
    "\n",
    "# scraping suicide_watch data\n",
    "depression_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/depression.json\", 50, depression_data)\n",
    "\n",
    "depression_data_unique = []\n",
    "create_unique_list(depression_data, depression_data_unique)\n",
    "\n",
    "# add suicide_watch to dataframe\n",
    "depression = pd.DataFrame(depression_data_unique)\n",
    "depression[\"is_suicide\"] = 0\n",
    "depression.head()\n",
    "\n",
    "# saving data\n",
    "suicide_watch.to_csv('suicide_watch.csv', index = False)\n",
    "depression.to_csv('depression.csv', index = False)\n",
    "\n",
    "# creating combined CSV\n",
    "depression = pd.read_csv('depression.csv')\n",
    "suicide_watch = pd.read_csv('suicide_watch.csv')\n",
    "\n",
    "dep_columns = depression[[\"title\", \"selftext\", \"author\",  \"num_comments\", \"is_suicide\",\"url\"]]\n",
    "sui_columns = suicide_watch[[\"title\", \"selftext\", \"author\",  \"num_comments\", \"is_suicide\",\"url\"]]\n",
    "\n",
    "combined_data = pd.concat([dep_columns, sui_columns],axis=0, ignore_index=True)  \n",
    "combined_data[\"selftext\"].fillna(\"emptypost\",inplace=True)\n",
    "combined_data.head()\n",
    "combined_data.isnull().sum()\n",
    "\n",
    "# saving combined CSV\n",
    "combined_data.to_csv('suicide_vs_depression.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
